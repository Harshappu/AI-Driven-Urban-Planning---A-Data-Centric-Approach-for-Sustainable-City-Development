{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YBTsYRsKDr4V"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import joblib\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('world_bank_open.csv')\n",
        "\n",
        "# Drop columns with a high percentage of missing values\n",
        "# Removed 'projectdoc', 'majorsector_percent', and 'theme' as they are not present in the DataFrame\n",
        "df = df.drop(columns=[\n",
        "    'mjsector1', 'mjsector2', 'mjsector3',\n",
        "    'mjsector4', 'mjsector5', 'mjtheme1name', 'mjtheme2name', 'mjtheme3name',\n",
        "    'mjtheme4name', 'mjtheme5name', 'Unnamed: 56', 'Country'\n",
        "])\n",
        "\n",
        "# Handle missing values\n",
        "df = df.dropna(subset=['lendprojectcost', 'ibrdcommamt', 'idacommamt', 'totalamt', 'grantamt'])\n",
        "\n",
        "# Convert columns with financial data to numeric types\n",
        "financial_cols = ['lendprojectcost', 'ibrdcommamt', 'idacommamt', 'totalamt', 'grantamt']\n",
        "df[financial_cols] = df[financial_cols].replace('[\\$,]', '', regex=True).astype(float)\n",
        "\n",
        "# Fill missing values in categorical columns with 'Unknown'\n",
        "categorical_cols = ['lendinginstr', 'lendinginstrtype', 'envassesmentcategorycode', 'supplementprojectflg',\n",
        "                    'productlinetype', 'projectstatusdisplay', 'status', 'borrower', 'impagency', 'sector',\n",
        "                    'mjsector', 'goal', 'financier', 'location']\n",
        "df[categorical_cols] = df[categorical_cols].fillna('Unknown')\n",
        "\n",
        "# Convert categorical variables to one-hot encoding\n",
        "categorical_features = categorical_cols\n",
        "\n",
        "# Include regionname in the categorical features\n",
        "categorical_features = categorical_cols + ['regionname']\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "], remainder='drop') # Drop any remaining columns not specified in the transformer\n",
        "\n",
        "# Define features and target variable\n",
        "X = df.drop(columns=['id', 'project_name', 'url', 'boardapprovaldate', 'board_approval_month', 'closingdate'])\n",
        "y = df['totalamt']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Preprocess the features using the preprocessor pipeline\n",
        "X_train = preprocessor.fit_transform(X_train)\n",
        "X_test = preprocessor.transform(X_test)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler(with_mean=False) # Set with_mean=False to disable centering\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Linear Regression Model\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Train Gradient Boosting Regression Model\n",
        "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
        "gb_model.fit(X_train, y_train)\n",
        "\n",
        "# Save the models\n",
        "joblib.dump(lr_model, 'linear_regression_model.pkl')\n",
        "joblib.dump(gb_model, 'gradient_boosting_regression_model.pkl')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaBlYxIcECep",
        "outputId": "8ed19776-1727-4cb8-cb57-07cfaee7986f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['gradient_boosting_regression_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define GAN components\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Hyperparameters\n",
        "input_dim = X_train.shape[1]\n",
        "output_dim = 1\n",
        "lr = 0.0002\n",
        "num_epochs = 40\n",
        "\n",
        "# Initialize models and move to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "generator = Generator(input_dim, output_dim).to(device)\n",
        "discriminator = Discriminator(input_dim + output_dim).to(device)\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=lr)\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Prepare data for GAN and move to GPU\n",
        "X_train_tensor = torch.tensor(X_train.toarray(), dtype=torch.float32).to(device)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Training loop for GAN\n",
        "for epoch in range(num_epochs):\n",
        "    for real_data, real_labels in train_loader:\n",
        "        batch_size = real_data.size(0)\n",
        "\n",
        "        # Move real data and labels to GPU\n",
        "        real_data = real_data.to(device)\n",
        "        real_labels = real_labels.view(-1, 1).to(device)\n",
        "\n",
        "        # Real data\n",
        "        real_input = torch.cat((real_data, real_labels), dim=1)\n",
        "        real_target = torch.ones(batch_size, 1).to(device)\n",
        "\n",
        "        # Fake data\n",
        "        noise = torch.randn(batch_size, input_dim).to(device)\n",
        "        fake_labels = generator(noise).detach()\n",
        "        fake_input = torch.cat((noise, fake_labels), dim=1)\n",
        "        fake_target = torch.zeros(batch_size, 1).to(device)\n",
        "\n",
        "        # Train Discriminator\n",
        "        optimizer_D.zero_grad()\n",
        "        real_loss = criterion(discriminator(real_input), real_target)\n",
        "        fake_loss = criterion(discriminator(fake_input), fake_target)\n",
        "        d_loss = real_loss + fake_loss\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Train Generator\n",
        "        optimizer_G.zero_grad()\n",
        "        generated_labels = generator(noise)\n",
        "        g_input = torch.cat((noise, generated_labels), dim=1)\n",
        "        g_loss = criterion(discriminator(g_input), real_target)\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch}, D Loss: {d_loss.item()}, G Loss: {g_loss.item()}')\n",
        "\n",
        "# Save the GAN models\n",
        "torch.save(generator.state_dict(), 'gan_generator.pth')\n",
        "torch.save(discriminator.state_dict(), 'gan_discriminator.pth')\n",
        "\n",
        "# Generate synthetic data\n",
        "with torch.no_grad():\n",
        "    synthetic_noise = torch.randn(X_train.shape[0], input_dim).to(device)\n",
        "    synthetic_labels = generator(synthetic_noise).cpu().numpy()  # Move back to CPU for further processing\n",
        "    synthetic_data = np.hstack((synthetic_noise.cpu().numpy(), synthetic_labels))\n",
        "\n",
        "# Augment original data with synthetic data\n",
        "augmented_X_train = np.vstack((X_train.toarray(), synthetic_data[:, :-1]))\n",
        "augmented_y_train = np.hstack((y_train, synthetic_data[:, -1]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKtONoLLEcj7",
        "outputId": "82e56791-8374-4a36-da73-d62d947d66d6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, D Loss: 81.91875457763672, G Loss: 6.274112224578857\n",
            "Epoch 10, D Loss: 83.6463623046875, G Loss: 28.164180755615234\n",
            "Epoch 20, D Loss: 85.45469665527344, G Loss: 46.66402816772461\n",
            "Epoch 30, D Loss: 94.54573059082031, G Loss: 36.662559509277344\n"
          ]
        }
      ]
    }
  ]
}